# GoogLeNet
## GoogLeNet Introduction
四个路径从不同层面抽取信息，然后在输出通道微合并
第一个超过100层的神经网络
**Inception块**
![图 1](assest/%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C/IMG_20220908-201619772.png) 

我们观察这四条路的输出和输入的形状相同。在输出通道处合并。注意，每条路上通道数可能不同

- 跟但3x3或5x5卷积层比，Inception块有更少的参数个数和计算复杂度

### 段1&2
- 更小的宽口，更多的通道
![图 2](assest/%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C/IMG_20220908-205316645.png)  

### 段3
![图 4](assest/%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C/IMG_20220908-205638946.png)  


## Inception变式
**Inception V3**
![图 6](assest/%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C/IMG_20220908-210808622.png)  



## 总结
- Inception块使用4条有不同超参数的卷积层和池化层的路来抽取不同的信息
  - 它的一个主要的优点是模型参数小，计算复杂度低
- GoogLeNet使用了9个Inception块，是一个达到上百层的网络(并不是指深度为100，而是指并行的层数有100)
  - 后续有一系列改进
