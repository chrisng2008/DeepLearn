# 批量归一化

- 损失出现在最后，后面的层训练较快
- 数据在最底部
  - 底部的层训练较慢
  - 底部层一变化，所有都得跟着变
  - 最后的拿些层需要重新学习多次
  - 导致收敛变慢

## Batch Normalization Introduction
- 固定小批量里面的均值和方差
  $$
    \mu_B = \frac{1}{|B|}\sum_{i \in B} x_i \text{\ and\ } \sigma_B^2 = \frac{1}{|B|}\sum_{i \in B}(x_i - \mu_B)^2 + \epsilon
  $$
  然后在做额外的调整(可学习的参数)
  $$
    x_{i+1} = \gamma \frac{x_i - \mu_B}{\sigma_B} + \beta
  $$
  其中，$\gamma$ 是方差，$\beta$ 是均值

### 批量归一化层
- 可学习的参数为$\gamma$ 和 $\beta$
- 作用在
  - 全连接层和卷积层输出上，激活函数前
  - 全连接层和卷积层上
- 对全连接层，作用在(特征维)上。
  - 对与输入矩阵来说，每一行是输入，每一列是特征。
  - 会对每一个特征计算均值和方差(进行归一化处理)，也会对学到的均值和方差对与原来的数据进行校认。
- 对与卷积层，作用在通道维
  - 像素作为样本， 通道作为特征
  - 我们可以想象成把卷积层看成拉成一个二维向量，像素作为样本，通道数作为特征

## 批量归一化在做什么
- 最初论文是想用它来减少内部协变量转移
- 后续有论文指出它可能是通过在每个小批量里加入噪音来控制模型复杂度(变量变化得不会很剧烈)
  $$
    x_{i+1} = \gamma \frac{x_i - \hat{\mu}_B}{\hat{\sigma}_B} + \beta
  $$
- 因此没必要跟丢弃法混合使用


## 总结
- 批量归一化固定小批量中的均值和方差，然后学习处适合的偏移和缩放
- 可以加快收敛速度，但一般不改变模型精度。