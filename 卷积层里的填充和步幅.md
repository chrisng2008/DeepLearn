# 卷积层里的填充和步幅

## 填充
- 给定(32 x 32)输入图像
- 应用 5 x 5大小的卷积核
  - 第1层得到输出大小28 x 28
  - 第7层得到输出大小 4 x 4(在上一层输出的基础上做卷积)
- 更大的卷积核可以更快地减少到输出大小
  - 形状从$n_h \times n_w$减少到$(n_h - n_k + 1)\times (n_w - k_w + 1)$

填充就是为了解决使用更大的卷积核而发生输出快速减小的问题。
填充的原理是，我们在输入的边缘中填充部分的参数。
![图 1](assest/%E5%8D%B7%E7%A7%AF%E5%B1%82%E9%87%8C%E7%9A%84%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85/IMG_20220904-222820692.png)  

- 填充$p_h$行和$p_w$列，输出形状为
    $$
        (n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)
    $$
- 通常取$p_h = k_h - 1,\  p_w = k_w - 1$(好处是输入和输出的大小相同)
  - 当$k_h$为奇数：在上下两侧填充$p_h/2$
  - 当$k_h$为偶数：在上侧填充$\left \lceil p_h/2 \right \rceil $, 在下侧填充$\left \lfloor p_h/2 \right \rfloor $

- 填充减小的输出大小和层数线性相关
  - 给定输入大小224 x 224，在使用5 x 5 卷积核的情况下，需要55层将输出降低到4x4((224-4)/4=55)
  - 需要大量的计算才能得到较小输出

## 步幅
- 步幅是指行/列的滑动步长
  - 例如高度3 宽度2 的步幅
- 给定高度$s_h$和宽度$s_w$的步幅，输出形状是
    $$
      \left \lceil (n_h - k_h + p_h + s_h)/s_h \right \rceil \times \left \lceil (n_w - k_w + p_w + s_w)/s_w \right \rceil
    $$
- 如果$p_h=k_h - 1$, $p_w = k_w - 1$
    $$
      \left \lceil (n_h + s_h - 1)/s_h \right \rceil \times \left \lceil (n_w + s_w - 1)/s_w \right \rceil
    $$
- 如果输出高度和宽度可以被步幅整除
  $$
    (n_h / s_h) \times (n_w / s_w)
  $$

### 总结
- 填充和步幅是卷积层的超参数
- 填充在输入周围添加额外的行/列，来控制输出形状的减少量
- 步幅是每次滑动核窗口时的行/列的步长，可以成倍的减少输出形状



### Q&A
- 超参数的选择
  1. 如果图片较大，通常需要计算很多层才能使图片降到比较大，此时我们可以调整步幅，稍微增大步幅，然后下降的就会比较快。步幅通常取2, 每次减半
  2. 填充通常是为了输出和输出一样，即填充的大小为核大小-1, 计算比较方便
  3. 核大小最为关键
  4. 卷积核的边长一般取奇数, 其实是偶数也行。选奇数是为了让上下填充比较对称
  5. 一般卷积处理完成后，维度通常不变。不能够每一次卷积后维度都减半，因为减半后，再做几层卷积，就没了，就不能做更深层次的卷积了

1. 卷积核的数量很多，但是我们通常不会自己取设计卷积核，都是套用经典的神经网络结构。除非输入十分特别，我们才会自己取设计卷积核，否则都是直接套用经典的卷积核，或者是在经典的架构上稍作调整。现在很多时候都是使用ResNet残差网络或者ResNet的变种。
2. 在Pytorch中，paddings=1表示我们在两边都同时增加1
3. 为什么我们通常用小的卷积核，例如3x3?因为我们使用小的卷积核，计算代价相对来说比较小。我们使用比较小的卷积核，意味着我们需要用更多的层才能获得最后一层的结果，才能看到整张图片里面的信息
4. 可以使用nas来同时学习其他的参数，但是很"贵"。有点类似与网格搜索，计算代价非常大
5. 通过多层卷积最后输出和输入形状相同，特征会丢失。机器学习本质上就是对信息筛选过滤的一个过程，信息是一定丢失的