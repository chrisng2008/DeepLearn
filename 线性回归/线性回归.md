[toc]
# 线性回归
成交价是关键因素的加劝和
$$
y = w_1x_1 ＋ w_2x_2 + w_3x_3 + b
$$
## 线性模型
- 给定n维输入$x=[w_1, w_2, \cdots, w_3], b$
- 输出是输入的加权和
$$
y = w_1x_1 ＋ w_2x_2 + w_3x_3 + b
$$

- 向量版本
$$
y=<w,x> + b
$$

**线性模型可以看作是单层神经网络**
![图 1](assest/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/IMG_20220830-172127464.png)  

### 衡量预测质量
假设y是真是值，$\hat{y}$是估计值，我们可以比较
$$
\ell (y, \hat{y}) = \frac{1}{2}(y-\hat{y})^2
$$
这个叫做平方损失

### 训练数据
$$
X = [x_1, x_2, \cdots, x_n]^T \\
y = [y_1, y_2, \cdots, y_n]^T
$$
> 默认是列向量，为了方便展示，我们通常用横向量的转置作为列向量
### 参数学习
- 训练损失
    $$
        \ell (X, y, w, b) = \frac{1}{2n} \sum_{i=1}^n (y_i - \left \langle x_i, w \right \rangle  - b)^2 = \frac{1}{2n} \left \| y-Xw-b \right \|^2
    $$
- 最小化损失来学习参数
    $$
        w^*, b^* = \text{arg}\min_{w,b}\ell (X, y, w, b)
    $$

- 将偏差加入权重 $\text{X} \leftarrow [X, 1]\ \ w \leftarrow \begin{bmatrix} w \\b \end{bmatrix} $
$$
\ell (\text{X, y, w}) = \frac{1}{2n}\left \| \text{y-Xw} \right \|^2 \\
\frac{\partial}{\partial w} = \ell(\text{X, y, w}) = \frac{1}{n}(y-Xw)^TX
$$

- 损失是凸函数，所以最优解满足
  $$
    \frac{\partial}{\partial \text{w}}\ell (\text{X, y, w}) = 0\\
    \Leftrightarrow \frac{1}{n} (y- \text{Xw})^TX=0 \\
    \Leftrightarrow w^* = (X^TX)^{-1} X^Ty
  $$

### 总结
- 线性回归是对n维输入的加权，外加偏差
- 使用平方损失来衡量预测值和真实值的差异
- 显示回归有显示解
- 线性回归可以看作是单层神经网络


## 基础优化方法
### 梯度下降
- 挑选一个初始值$w_0$
- 重复迭代参数t=1,2,3
    $$
        w_t = w_{t-1} = - \eta \frac{\partial \ell}{\partial w_{t-1}}
    $$
    - 沿梯度的方向将增加损失函数值
    - 学习率$\ell$：步长的超参数

![图 2](assest/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/IMG_20220830-180743438.png)  

学习率实际上指的是步长，步长要适中，不能太大，也不能太小
- 如果学习率太小，会导致更新太慢
- 如果学习率太大，有可能找不到最小值,一直在震荡
  ![图 3](assest/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/IMG_20220830-191912054.png)  

#### 自适应学习率
学习率大小，正确选择学习率是一个很重要的因素，我们通常采用自适应学习率的方法来选择学习率

### 小批量随机梯度下降 mini-batch
- 在整个训练集上算梯度太贵
  - 一个深度神经网络模型可能需要数分钟至数小时
- 我们可以随机采样b个样本$i_1, i_2, \cdots, i_b$来近似损失
    $$
        \frac{1}{b}\sum_{i\in I_b}\ell (x_i, y_i, w)
    $$
  - b是批量大小，另一个重要的超参数
> b不能太小：每次计算量太小，不适合并行来最大利用计算资源
> 
> b不能太大：内存消耗增加浪费计算，例如如果所有的样本都是相同的

##
