# 丢弃法
- 一个好的模型需要对输入数据的扰动鲁棒
  - 使用有噪音的数据等价于Tikhonov正则
  - 丢弃法：在层之间加入噪音

## 无偏差的加入噪音
- 对x加入噪音得到$x'$, 我们希望
  $$
    E(x') = x
  $$
- 丢弃法对每个元素进行如下扰动
    $$
        x_i' = \begin{cases}
            0 & \text{with\ probablity} \ p \\
            \frac{x_i}{1-p} &\text{otheriwise}
        \end{cases}
    $$
实际上，就是以一定的概率丢弃节点。例如
$$
  E(x_i) = p \cdot 0 + (1-p) \cdot \frac{x_i}{1-p}
$$
计算出来，期望并没有发生变化。

- 使用丢弃法
  - 通常将丢弃法作用在隐藏层全连接层的输出上

```python
h = sigma(W_1 * x + b_1)
h' = dropout(h)
o = W_2 * h' + b_2
y = softmax(o)
```

![图 1](assest/%E4%B8%A2%E5%BC%83%E6%B3%95/IMG_20220902-224940790.png)  
使用了暂退了，隐藏层减少了部分节点。虽说节点的数量减少了，但是却能够减少过拟合的发生，即避免了输出过于依赖隐藏层的每个特征


### 推理中的丢弃法
- 正则项只在训练中使用：他们影响参数模型的更新
- 在推理过程中，丢弃法直接返回输入
  $$
    h = \text{dropout}(h)
  $$
  - 这样也能保证确定性的输出

> dropout可以当成一个正则项，只在训练中使用, 只会对权重产生影响。我们随机丢弃部分神经元，将训练出来的神经网络做平均，结果会有所改善

## 总结
- 丢弃法将一些输出项随机设置0来控制模型复杂度
- 常作用在多层感知机的隐藏层输出上
- 丢弃概率是控制模型复杂度的超参数


**dropout这个方法十分重要，是现在最为主流的多层感知机的控制方法**