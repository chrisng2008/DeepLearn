# 数值稳定性

## 数值稳定性常见的两个问题

- 梯度爆炸
    使用ReLU作为激活函数
    $$
    \sigma(x) = \max (0, x) \text{\ \ and\ \ } \sigma'(x) = \begin{cases}
       1 & \text{if} \ \ x > 0\\ 
       0 & \text{otherwise}
    \end{cases}
    $$
**梯度爆炸的问题**
- 值超出值域(infinity)
    - 对于16位浮点数尤为严重(数值区间 6e-5 - 6e4)
- 对学习率敏感
    - 如果学习率太大 -> 大参数值 -> 更大的梯度
    - 如果学习率太小 -> 训练无进展
    - 我们可能需要在训练的过程中不断调整学习率
- 梯度消失
- 使用sigmoid作为激活函数
    $$
        \sigma(x) = \frac{1}{1+e^{-x}}(1-\sigma(x))
    $$
    ![图 1](assest/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/IMG_20220903-104644780.png)  
    sigmoid 函数中间的梯度大，两端的梯度小
  
**梯度消失的问题**
- 梯度值变成0
  - 对16位浮点数尤为严重
- 训练没有进展
  - 不管如何选择学习率
- 对于底部层尤为严重
  - 仅仅顶部层训练的较好
  - 无法让神经网络更深

## 总结
- 当数值过大或过小时会导致数值问题
- 常发生在深度模型中，因为其会对n个数累乘