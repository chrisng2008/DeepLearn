# 数值稳定性

## 数值稳定性常见的两个问题

- 梯度爆炸
    使用ReLU作为激活函数
    $$
    \sigma(x) = \max (0, x) \text{\ \ and\ \ } \sigma'(x) = \begin{cases}
       1 & \text{if} \ \ x > 0\\ 
       0 & \text{otherwise}
    \end{cases}
    $$
**梯度爆炸的问题**
- 值超出值域(infinity)
    - 对于16位浮点数尤为严重(数值区间 6e-5 - 6e4)
- 对学习率敏感
    - 如果学习率太大 -> 大参数值 -> 更大的梯度
    - 如果学习率太小 -> 训练无进展
    - 我们可能需要在训练的过程中不断调整学习率
- 梯度消失
- 使用sigmoid作为激活函数
    $$
        \sigma(x) = \frac{1}{1+e^{-x}}(1-\sigma(x))
    $$
    ![图 1](assest/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/IMG_20220903-104644780.png)  
    sigmoid 函数中间的梯度大，两端的梯度小
  
**梯度消失的问题**
- 梯度值变成0
  - 对16位浮点数尤为严重
- 训练没有进展
  - 不管如何选择学习率
- 对于底部层尤为严重
  - 仅仅顶部层训练的较好
  - 无法让神经网络更深

### 总结
- 当数值过大或过小时会导致数值问题
- 常发生在深度模型中，因为其会对n个数累乘

## 让训练更加稳定
- 目标：让梯度值在合理的范围内
- 将乘法变加法
  - ResNet, LSTM
- 归一化
  - 梯度归一化，梯度裁剪
- 合理的权重初始和激活函数

- 让煤层的输出和梯度都看作随机变量
- 让他们的均值和方差都保持一致

正向
$$
E[h_i'] = 0 \\
\text{Var}[h_i^t] = a
$$

反向
$$
E[\frac{\partial \ell}{\partial h_i^t}] = 0 \\
\text{Var}[\frac{\partial \ell}{\partial h_i^t}] = b
$$

a, b 都是常数

### 权重初始化
- 在合理值区间里随机设置初始参数
- 训练开始的时候更容易有数值不稳定
  - 原理最优解的地方损失函数表明可能很复杂
  - 在最优解附近表面会比较平
- 使用 $\mathcal{N}(0, 0.01)$来初始可能对小网络没问题，但不能保证深度神经网络

#### 检查常用激活函数
- 使用泰勒公式展开
  $$
  \text{sigmoid}(x) = \frac{1}{2} + \frac{x}{4} - \frac{x^3}{48} + O(x^5) \\
  \tanh(x) = 0 + x - \frac{x^3}{3} + O(x^5) \\
  \text{relu}(x) = 0 + x \ \ \ \text{for} \ \ x \geq 0
  $$
使用tanh或者relu作为激活韩式都是可以的。用sigmoid作为激活函数可能会有梯度消失的现象发生。我们可以对sigmoid进行变化
- 调整sigmoid
  $$
    4 \times \text{sigmoid}(x) - 2
  $$
**总的来说，最常使用的激活函数就是sigmoid**

#### 总结
- 合理的权重初始值和激活函数的选取可以提升数值稳定性