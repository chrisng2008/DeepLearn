# 自动求导
## 求导
求导通常遵循链式法则，例如
$$
a = Xw\\
b=a-y\\
z=||b||^2
$$


$$
\frac{\partial x}{\partial w}=\frac{\partial z}{\partial b}\frac{\partial b}{\partial a}\frac{\partial a}{\partial w}
$$

## 自动求导
但是神经网路的层数太多，因此手动求导过于复杂，因此我们通常使用自动求导。
- 自动求导计算一个函数在指定值上的导数
- 它有别于
    - 符号求导
        ```
        In[1]:D[4x^3+x^2+3, x]
        Out[1]= 2x+12x^2
        ```
    - 数值求导
        $$
        \frac{\partial f(x)}{\partial x}=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}
        $$
        对于数值求导，我们不需要知道函数的形式，我们只需要利用一个很小的数值去拟合函数

- 自动求导计算图
    - 将代码分解成操作子
    - 将计算表示成一个无关图
![figure](assest/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/IMG_20220829-170241122.png)

### 自动求导的两种模式
- 链式法则
$$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u_n}\frac{\partial u_n}{\partial u_{n-1}} \cdots \frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x}
$$
- 正向累积
$$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u_n}(\frac{\partial u_n}{\partial u_{n-1}} (\cdots \frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x}))
$$
- 反向累积、又称反向传递
$$
\frac{\partial y}{\partial x} = (((\frac{\partial y}{\partial u_n}\frac{\partial u_n}{\partial u_{n-1}}) \cdots )\frac{\partial u_2}{\partial u_1})\frac{\partial u_1}{\partial x}
$$
![figure2](assest/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/IMG_20220829-193225808.png)  


## 复杂度
- 计算复杂度：O(n), n是操作子个数
  - 通常正向和方向的代价类似
- 内存复杂度：O(n), 因为需要存储正向的所有中间结果
- 跟正向累积对比
  - O(n)计算复杂度用来计算一个变量的梯度
  - O(1)内存复杂度


## PyTorch自动求导
### 举例1
在Pytorch中，我们通常会使用一个变量来存储梯度,例如下面的例子我们使用变量x来存储梯度
```python
x = torch.tensor(4.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=False)
z = torch.pow(x, 2) + torch.pow(y, 2)

z.backward()
print(x.grad())     # tensor(6.)
print(y.grad())     # None
```
### 举例2
#### 01
要理解以下的dot点乘，需要用到线性代数中点乘的知识点
$$
a \cdot b = \sum_{i=1}^{n} a_i \cdot b_i
$$
```python
x = torch.arange(4.0)
x.requires_grad_(True)
x.grad # 默认值为None
x
# Out: tensor([0., 1., 2., 3.], requires_grad=True)
y = 2 * torch.dot(x, x)
y
# Out: tensor(28., grad_fn=<MulBackward0>)
# 求导
# 通过反向传播函数来自动计算y关于x的每个分量的梯度
y.backward()
x.grad
# Out: tensor([ 0.,  4.,  8., 12.])
```
$$
y = 2 \cdot x \cdot x \\
x \cdot x = 0 \cdot 0 + 1 \cdot 1 + 2 \cdot 2 + 3 \cdot 3 = 0 + 1 + 4 + 9 = 14\\
y^{'} = 4x = [0, 4, 8, 12]
$$


#### 02
需要注意的是，Pytorch会累积梯度，因此我们需要清除之前的值
现在我们求解的是x.sum()的梯度。我们可以分析一下，实际上x.sum()相当于x与E常矩阵做点乘的结果
$$
\begin{aligned}
 x.sum() = x \cdot E &= [a_1, a_2, a_3, a_4] \cdot [1, 1, 1, 1] \\
&= a_1 \cdot 1 + a_2 \cdot 1 + a_3 \cdot 1 + a_4 \cdot 1   \\
&= a_1 + a_2 + a_3 + a_4
\end{aligned}
$$
```python
x.grad.zero_()
y = x.sum()
y.backward()
x.grad()
# Out: tensor([1., 1., 1., 1.])
```


#### 03
前面我们计算梯度，都是对标量来进行求梯度的，但是，如果我们遇到非标量怎么办，我们可以对非标量进行sum()运算(将非标量转化为标量)，然后在使用反向传播backward函数来进行梯度计算。
```python
# 对于非标量调用backward需要传入一个gradient参数
x.grad.zero_()
y = x * x
y.sum().backward()
x.grad
# Out: tensor([0., 2., 4., 6.])
```

#### 04
下面介绍我们可以使用detach函数让pytorch将某参数当作常数来计算
```python
x.grad.zero_()
y = x * x
# detach()是把y看作一个常数
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
# Out: tensor([True, True, True, True])
```
如果我们不把y看成常数，而是关于x的函数，则
```python
x.grad.zero_()
y = x * x
y.sum().backward()
x.grad == 2 * x
# Out: tensor([True, True, True, True])
```