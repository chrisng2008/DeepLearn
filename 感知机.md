# 感知机
- 给定输入x， 权重w， 和偏移b， 感知机输出：
  $$
    o=\sigma (\left \langle  w, x \right \rangle + b)\ \ \ \ \sigma(x) = \begin{cases}
       1 & \text{if}\ \ x > 0 \\
       0    & \text{otherwise}
    \end{cases}
  $$


- 二分类:   -1 或 1
  - Vs 回归输出实数
  - Vs Softmax 回归输出概率


训练感知机
```
initialize w = 0 and b = 0
repeat
    if y_i[<w, x_i> + b] <= 0 then
        w = w + y_i * x_i and b = b + y_i
    end if
until all classified correctly 
```

注意以上的伪代码, 如果$y_i [\left \langle w, x_i \right \rangle + b] \leq  0$, 表示我们分类错误，更新参数。需要注意的是，上面我们的式子是乘法。感知机的输出为1或0(-1)
等价于使用批量大小为1的梯度下降，并使用一下的损失函数。分类正确损失函数为0， 分类错误损失函数不为零，因此存在梯度，使用梯度下降法
$$
    \ell (y, x, w) = \max(0, -y \left \langle w, x \right \rangle)
$$
实际上，以上的损失函数的写法忽略了b，正确的写法应该如下
$$
\min_{w, b} L(w, b) = \sum_{x_i \in M} y_i (w \cdot x_i + b)
$$

为何更新w为$w = w + y_i * x_i$, 我们通过损失函数求导可得。

## 收敛定理
- 数据在半径r内
- 余量$\rho$分类两类
    $$
      y(w^T w + b) \geq \rho
    $$
    对于 $\left \| w \right \|^2 + b^2 \leq 1 $
- 感知机保证在$\frac{r^2 + 1}{\rho^2}$步后收敛

## XOR问题
**感知机不能拟合XOR函数，它只能产生线性分割面**
![图 1](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220831-230631567.png)  

由于普通感知机不能够解决XOR问题，因此后面演变出了能够解决XOR问题的感知机。

## 多层感知机
多层感知机的原理主要是，我们分别学习多个模型，然后将模型组合到一起，形成新的感知机模型
![图 2](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220831-230944555.png)  

![图 3](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220831-231027482.png)  

![图 4](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220831-231035740.png)  

### 单隐藏层 - 单分类
![图 5](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220831-231418216.png)  
隐藏层的大小为超参数
输入层的大小我们并不能改变

- 输入$x \in \mathbb{R}^n$
- 隐藏层 $W_1 \in \mathbb{R}^{m \times n}, b_1 \in \mathbb{R}^m$
- 输出层$w_2 \in \mathbb{R}^m, b_2 \in \mathbb{R}$
  $$
    h = \sigma(W_1x+b_1)  \\
    o = w_2^T h + b_2
  $$
  $\sigma$是按元素的激活函数, **激活函数一定是非线性的**
  如果激活函数非线性,那么$o=w_2^TW_1x+b'$， 仍然是线性,等价一个单层的感知机。

### Sigmoid激活函数
将输入投影到(0, 1)是一个软的$\sigma(x) = \begin{cases}
1 & \text{if} \ \ x>0 \\
0 &\text{otherwise} \end{cases}$
但是，上面的函数我们实际上很少用到，因为求导起来比较复杂(在0处不好求导)。我们通常激活函数使用sigmoid函数

$$
\text{sigmoid(x)} = \frac{1}{1+\exp{(-x)}}
$$

![Sigmoid函数图像](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220901-114810059.png)  

### Tanh激活函数
- 将输入投影到(-1, 1)
$$
\tanh (x) = \frac{1-\exp(-2x)}{1+\exp(-2x)}
$$
![tenh激活函数](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220901-121236137.png)  


### ReLU激活函数
ReLU: rectified linear unit
$$
  \text{ReLU}(x) = \max(x, 0)
$$
![图 3](assest/%E6%84%9F%E7%9F%A5%E6%9C%BA/IMG_20220901-123936431.png)  
RuLU激活函数更常用的原因：激活函数中没有指数运算，计算起来较为方便。因此一次指数运算在CPU中约等于计算100次乘法运算

## 多类分类
$$
y_1, y_2, \dots, y_k = \text{softmax} = (o_1, o_2, \dots, o_k)
$$


## 总结
- 多层感知机使用隐藏层和激活函数来得到非线性模型
- 常用的激活函数是Sigmoid， Tanh， ReLU
- 使用softmax来处理多类分类
- 超参数为隐藏层数，和各个隐藏层大小

多层感知机的重点是，我们改如何选择隐藏层的层数，节点的个数，这是我们需要去注意的。实际上，深度学习就是一个调参的过程