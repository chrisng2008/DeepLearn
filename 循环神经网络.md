# RNN

## 潜变量自回归模型
- 使用潜变量$h_t$总结过去信息

![图 1](assest/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/IMG_20221019-221638305.png)  


## 循环神经网络
- 更新隐藏状态: $h_t = \phi(w_{hh}h_{t-1} + W_{ht}x_{t-1} + b_h)$
    > 去掉第一项$w_{hh}h_{t-1}$就退化成了MLP
- 输出: $\phi(W_{ho}h_t + b_o)$
![图 2](assest/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/IMG_20221019-222837973.png)  
我们根据$h_t$就能够形成输出
更新隐藏状态: $h_t = \phi(w_{hh}h_{t-1} + W_{ht}x_{t-1} + b_h)$
新状态由前一个隐藏状态和输入共同生成
输出：$o_t=\phi(W_{ho}h_t+b_o)$
RNN实际上是通过$W_hh$来存储时序信息的

## 困惑度(perplexity)
- 衡量一个语言模型的好坏可以用平均交叉熵
$$
\pi = \frac{1}{n}\sum_{i=1}^{n}-\log p(x_t | x_{t-1}, \cdots)
$$
p是语言模型的预测概率，$x_t$是真实值
- 历史原因NLP使用困惑度$\exp(\pi)$来衡量，是平均每次可能选项
  - 1表示完美，无穷大是最差情况

## 梯度裁剪
- 迭代中计算T个时间步上的梯度，在反向传播过程中产生长度为$O(T)$的矩阵乘法链，导致数值不稳定
- 梯度裁剪能有效预防梯度爆炸(太多矩阵乘法可能会导致梯度爆炸)
  - 如果梯度长度超过$\theta$, 那么拖影回长度$\theta$(这样可以使长度永远不会超过 $\theta$ )
$$
    g \leftarrow \min (1, \frac{\theta}{||g||})g
$$

## RNNs的应用
![图 3](assest/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/IMG_20221019-232627237.png)  

## 总结
- 循环神经网络的输出取决于当前输入和前一时间的隐变量
- 应用到语言模型中时，循环神经网络根据当前词预测下一次时刻词
- 通常使用困惑度来衡量语言模型的好坏

## Q&A
1. RNN只能够处理一些序列模型
2. 自然语言处理，目标检测，图片分类发好论文比较难, 现在比较热门的模块是Transformer模型
3. 如果一个人发论文，可以选择使用transformer偏应用的方向去发表论文
4. 现在多模态融合是比较热门的